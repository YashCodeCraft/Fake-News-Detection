# -*- coding: utf-8 -*-
"""3. Fake News Detection (NLP).ipynb

Automatically generated by Colab.


"""

import pandas as pd
import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("dataset in csv format")
# print(df)

df = df[:50]
# print(df)

df.info()

df.dropna(inplace=True)

df.isnull().sum()

df.shape

df.label.value_counts()

inputt = df['text']
outputt = df['label']

messages = df.copy()
messages.reset_index(inplace=True, drop=True)
messages.head()

messages.shape

messages['text'][0]

lemma = WordNetLemmatizer()
corpus = []
for i in range(len(messages)):
    news = re.sub('[^a-zA-Z]', " ", messages['text'][i])
    news = news.lower()
    news = news.split()
    sentences = [lemma.lemmatize(j, pos='v') for j in news
                 if j not in stopwords.words('english')]
    sentences = " ".join(sentences)
    corpus.append(sentences)

# print(corpus)

vector = CountVectorizer(max_features=5000, ngram_range=(1, 3))
matrix = vector.fit_transform(corpus).toarray()
print(matrix)

matrix.shape

vector.get_feature_names_out()

corpus[:1]

df = df.drop(columns = ['id', 'title', 'author', 'text'])

df_cv = pd.concat([df, pd.DataFrame(matrix)], axis=1)
df_cv.dropna(inplace=True)

df_cv

x_train, x_test, y_train, y_test = train_test_split(df_cv.drop(columns=['label']), df_cv['label'])

nb = MultinomialNB()
nb.fit(x_train, y_train)

y_pred = nb.predict(x_test)
y_pred

print(classification_report(y_pred, y_test))

print(confusion_matrix(y_pred, y_test))

plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cmap='YlGnBu')
plt.show()

